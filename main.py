# -*- coding: utf-8 -*-
"""Sales Copilot Architecture.ipy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169FsVVq1ZC6OHzd75JRHZTDuICM4mtHS
"""

"""
Judgment-Driven Sales Copilot (MVP)
Author: Zimkitha Ntshikaniso
Date: 23 December 2025
Description:
    A deterministic RAG pipeline for Enterprise Sales data.
    Implements Regex-based PII sanitization ('The Laundromat') prior to vectorization.
    Utilizes OpenAI GPT-4o (Temp=0) and Pinecone Serverless.
"""

# Install libraries (Colab specific command)
!pip install -qU pinecone-client openai

import os
import time
import re
from getpass import getpass
from openai import OpenAI
from pinecone import Pinecone, ServerlessSpec

# --- Configuration & Auth ---
# TODO: Move API keys to environment variables for production deployment
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass("Enter OpenAI API Key: ")

if "PINECONE_API_KEY" not in os.environ:
    os.environ["PINECONE_API_KEY"] = getpass("Enter Pinecone API Key: ")

INDEX_NAME = "sales-copilot"

# Initialize clients
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])

# Check/Create Vector Index
existing_indexes = [index['name'] for index in pc.list_indexes()]

if INDEX_NAME not in existing_indexes:
    print(f"Index '{INDEX_NAME}' not found. Initializing new index...")
    pc.create_index(
        name=INDEX_NAME,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
    time.sleep(10) # Buffer for index propagation
else:
    print(f"Connected to index: '{INDEX_NAME}'")

index = pc.Index(INDEX_NAME)


# --- Core Logic Functions ---

def sanitization_layer(text):
    """
    Pre-processing layer to strip PII before embedding.
    Ensures GDPR compliance by removing emails and internal IDs.
    """
    # Pattern 1: Redact Emails
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[REDACTED_EMAIL]', text)

    # Pattern 2: Redact Slack/User IDs
    text = re.sub(r'<@[A-Z0-9]+>', '[REDACTED_USER_ID]', text)

    return text

def generate_embedding(text):
    """Wrapper for OpenAI embedding API."""
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding


# --- Ingestion Pipeline ---

# Simulated Slack dump
raw_data_batch = [
    # NOTE: Sarah Jones is the name (Public), <@U888> is the ID (Private/Redacted)
    {"id": "msg_001", "text": "The deal with Acme Corp is worth $50k. Sarah Jones (<@U888>) is the lead."},
    {"id": "msg_002", "text": "Beta Inc requires a 20% discount. Email approval to boss@company.com."},
    {"id": "msg_003", "text": "Gamma LLC signed the NDA yesterday. We start the pilot next week."},
    {"id": "msg_004", "text": "Competitor X is undercutting us on the Delta project by $5k."},
    {"id": "msg_005", "text": "Meeting with Omega Co canceled. Reschedule for Q4."}
]

print(f"\nStarting batch ingestion...")

for record in raw_data_batch:
    # 1. Sanitize
    clean_payload = sanitization_layer(record['text'])

    # Debug logging for sanitation check
    if clean_payload != record['text']:
        print(f"Sanitizing record {record['id']}...")

    # 2. Embed
    vector = generate_embedding(clean_payload)

    # 3. Upsert with metadata
    metadata = {"content": clean_payload, "source": "slack_dump"}
    index.upsert(vectors=[(record['id'], vector, metadata)])

time.sleep(2)
print("Batch ingestion complete.")


# --- RAG Inference ---

def query_sales_engine(user_query):
    print(f"\nUser Query: {user_query}")

    # 1. Vector Search
    query_vec = generate_embedding(user_query)
    results = index.query(vector=query_vec, top_k=2, include_metadata=True)

    # 2. Context Assembly
    context_block = "\n".join([match['metadata']['content'] for match in results['matches']])
    print(f"Retrieved Context: {context_block}")

    # 3. LLM Response Generation
    # Using Temp=0 to enforce deterministic output based strictly on context
    system_instruction = f"""
    You are a Sales Operations Assistant.
    Answer strictly based on the provided CONTEXT.
    If the answer is missing, state: "Data not available in knowledge base."

    CONTEXT:
    {context_block}
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_instruction},
            {"role": "user", "content": user_query}
        ],
        temperature=0
    )

    return response.choices[0].message.content


# --- Integration Tests ---

# Test 1: Info Retrieval (Should say "Sarah Jones")
print(f"ðŸ¤– Response: {query_sales_engine('Who is the lead on the Acme deal?')}")

# Test 2: PII Redaction Check (Should NOT reveal boss@company.com)
print(f"ðŸ¤– Response: {query_sales_engine('Where do I email approval for Beta Inc?')}")

# Test 3: Data Safety (Negative Test)
print(f"ðŸ¤– Response: {query_sales_engine('What is the weather in Tokyo?')}")