{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAOv+cE0z0DShTo+YkZTiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZIMKITH/Judgement-driven-sales-copilot/blob/main/Sales_Copilot_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yy7ypohFsWY"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# --- SECURE CREDENTIAL INPUT ---\n",
        "print(\"üîê Please enter your credentials (input is hidden for security):\")\n",
        "\n",
        "# 1. OpenAI Setup\n",
        "# When you run this, a box will appear. Paste your key and hit Enter.\n",
        "OPENAI_API_KEY = getpass(\"Enter OpenAI API Key: \")\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# 2. Pinecone Setup\n",
        "PINECONE_API_KEY = getpass(\"Enter Pinecone API Key: \")\n",
        "PINECONE_INDEX_NAME = input(\"Enter your Pinecone Index Name (e.g., sales-copilot): \")\n",
        "\n",
        "# Initialize Pinecone Client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# 3. Connection Verification\n",
        "try:\n",
        "    # Check if the index exists in your project\n",
        "    existing_indexes = [index['name'] for index in pc.list_indexes()]\n",
        "    if PINECONE_INDEX_NAME in existing_indexes:\n",
        "        print(f\"\\n‚úÖ SUCCESS: Connected to OpenAI and Pinecone. Index '{PINECONE_INDEX_NAME}' found.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è WARNING: Connected to Pinecone, but Index '{PINECONE_INDEX_NAME}' was not found.\")\n",
        "        print(f\"Available indexes: {existing_indexes}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERROR: Connection failed. Check your API Key. \\nDetails: {e}\")"
      ],
      "metadata": {
        "id": "pCIcx5h_Hgb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# PHASE 2: THE \"LAUNDROMAT\" (INGESTION & CLEANING)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans unstructured text data for RAG ingestion.\n",
        "    1. Removes Slack User IDs (format: <@U12345>)\n",
        "    2. Redacts Email addresses\n",
        "    \"\"\"\n",
        "    # 1. Regex to remove Slack User IDs\n",
        "    # Logic: Look for '<@', followed by 'U', followed by any alphanumeric chars, ending with '>'\n",
        "    text = re.sub(r'<@U[A-Z0-9]+>', '', text)\n",
        "\n",
        "    # 2. Regex to redact Emails\n",
        "    # Logic: Look for standard email patterns and replace with placeholder\n",
        "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', '[EMAIL_REDACTED]', text)\n",
        "\n",
        "    # 3. Clean up extra whitespace created by removals\n",
        "    # Logic: Split by whitespace and rejoin with single spaces\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- ARCHITECTURAL TEST ---\n",
        "# We define messy test data to verify our logic works\n",
        "raw_slack_messages = [\n",
        "    \"Hey <@U025W>, did we send the contract to johndoe@acmecorp.com yet?\",\n",
        "    \"Reviewing the terms with <@U999X>. send feedback to legal@internal.org ASAP.\",\n",
        "    \"Deal is stuck. <@U1234> pls help.\"\n",
        "]\n",
        "\n",
        "print(\"--- TESTING THE LAUNDROMAT ---\")\n",
        "for msg in raw_slack_messages:\n",
        "    cleaned = clean_text(msg)\n",
        "    print(f\"üî¥ RAW:   {msg}\")\n",
        "    print(f\"üü¢ CLEAN: {cleaned}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "IABnga_qJmv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# PHASE 3: VECTORIZATION & STORAGE\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates vector embeddings using OpenAI's text-embedding-3-small model.\n",
        "    Output: A list of 1,536 floats.\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-3-small\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# --- KNOWLEDGE BASE (Simulated Sales Data) ---\n",
        "# We are creating 5 fake sales logs to put into the brain of the AI.\n",
        "sales_data = [\n",
        "    {\"id\": \"msg_001\", \"text\": \"The deal with Acme Corp is worth $50k. <@U888> is the lead.\"},\n",
        "    {\"id\": \"msg_002\", \"text\": \"Beta Inc requires a 20% discount. Email approval to boss@company.com.\"},\n",
        "    {\"id\": \"msg_003\", \"text\": \"Gamma LLC signed the NDA yesterday. We start the pilot next week.\"},\n",
        "    {\"id\": \"msg_004\", \"text\": \"Competitor X is undercutting us on the Delta project by $5k.\"},\n",
        "    {\"id\": \"msg_005\", \"text\": \"Meeting with Omega Co canceled. Reschedule for Q4.\"}\n",
        "]\n",
        "\n",
        "# Connect to the Pinecone Index\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "print(f\"üöÄ Starting Ingestion into Index: {PINECONE_INDEX_NAME}...\")\n",
        "\n",
        "# PROCESS LOOP: Clean -> Embed -> Upsert\n",
        "for item in sales_data:\n",
        "    # 1. Clean the text using your function\n",
        "    cleaned_text = clean_text(item['text'])\n",
        "\n",
        "    # 2. Generate Embedding (Turn text into numbers)\n",
        "    vector = get_embedding(cleaned_text)\n",
        "\n",
        "    # 3. Metadata (Store the text so we can read it later)\n",
        "    metadata = {\n",
        "        \"original_text\": cleaned_text,\n",
        "        \"source\": \"slack\"\n",
        "    }\n",
        "\n",
        "    # 4. Upsert to Pinecone\n",
        "    # Format: (Unique ID, Vector List, Metadata Dictionary)\n",
        "    index.upsert(vectors=[(item['id'], vector, metadata)])\n",
        "\n",
        "    print(f\"‚úÖ Indexed: {item['id']}\")\n",
        "\n",
        "# Small pause to ensure Pinecone processes the data\n",
        "time.sleep(2)\n",
        "print(\"\\nüéâ PHASE 3 COMPLETE: Data is now stored in the Vector Database.\")"
      ],
      "metadata": {
        "id": "yRG-7qGmKu8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 4: RETRIEVAL AUGMENTED GENERATION (RAG)\n",
        "\n",
        "def ask_sales_copilot(question):\n",
        "    \"\"\"\n",
        "    1. Searches the Vector DB for relevant info.\n",
        "    2. Sends context + question to GPT-4o.\n",
        "    \"\"\"\n",
        "    print(f\"\\n‚ùì QUESTION: {question}\")\n",
        "\n",
        "    # STEP A: Embed the Question\n",
        "    query_vector = get_embedding(question)\n",
        "\n",
        "    # STEP B: Retrieve relevant context from Pinecone\n",
        "    search_response = index.query(\n",
        "        vector=query_vector,\n",
        "        top_k=2,  # Get the top 2 most relevant matches\n",
        "        include_metadata=True\n",
        "    )\n",
        "\n",
        "    # Extract the text from the search results\n",
        "    contexts = [match['metadata']['original_text'] for match in search_response['matches']]\n",
        "    context_str = \"\\n\".join(contexts)\n",
        "\n",
        "    print(f\"üîç FOUND CONTEXT: {contexts}\")\n",
        "\n",
        "    # STEP C: Generate Answer with GPT-4o\n",
        "    system_prompt = f\"\"\"\n",
        "    You are a Sales Operations Copilot.\n",
        "    Answer the user's question based ONLY on the context provided below.\n",
        "    If the answer is not in the context, say \"I don't have that information.\"\n",
        "\n",
        "    CONTEXT:\n",
        "    {context_str}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- TEST DRIVE ---\n",
        "# Let's ask a question that requires knowledge from our hidden database.\n",
        "answer1 = ask_sales_copilot(\"How much is the Acme deal worth?\")\n",
        "print(f\"ü§ñ COPILOT ANSWER: {answer1}\")\n",
        "\n",
        "answer2 = ask_sales_copilot(\"What is happening with Beta Inc?\")\n",
        "print(f\"ü§ñ COPILOT ANSWER: {answer2}\")"
      ],
      "metadata": {
        "id": "25mH9IppLKpg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}